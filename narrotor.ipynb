{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c14b1aa8-d19e-4d67-a998-30116d07c5ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video duration: 42 seconds.\n",
      "Word Count: 92.4\n",
      "Frame extraction complete. 22 frames extracted.\n",
      "extracted_frames ['extracted_frames/frame_0.jpg', 'extracted_frames/frame_60.jpg', 'extracted_frames/frame_120.jpg', 'extracted_frames/frame_180.jpg', 'extracted_frames/frame_240.jpg', 'extracted_frames/frame_300.jpg', 'extracted_frames/frame_360.jpg', 'extracted_frames/frame_420.jpg', 'extracted_frames/frame_480.jpg', 'extracted_frames/frame_540.jpg', 'extracted_frames/frame_600.jpg', 'extracted_frames/frame_660.jpg', 'extracted_frames/frame_720.jpg', 'extracted_frames/frame_780.jpg', 'extracted_frames/frame_840.jpg', 'extracted_frames/frame_900.jpg', 'extracted_frames/frame_960.jpg', 'extracted_frames/frame_1020.jpg', 'extracted_frames/frame_1080.jpg', 'extracted_frames/frame_1140.jpg', 'extracted_frames/frame_1200.jpg', 'extracted_frames/frame_1260.jpg']\n",
      "MoviePy - Writing audio in extracted_audio.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 139M/139M [00:11<00:00, 12.5MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this intense Valorant match, we see a player strategically peeking around corners, keeping an eye on the enemy's movements. With only 50 seconds left, they spot an opponent and take a shot, landing a headshot! The tension rises as they navigate through tight spaces, using cover effectively. \n",
      "\n",
      "With the score at 4-5, every move counts. The player quickly assesses their surroundings, preparing for a potential ambush. As the clock ticks down, they make a decisive play, securing a clutch moment just in time! What a thrilling finish!\n",
      "Revised Description from Chat Completions API:\n",
      "In this thrilling 42-second Valorant match, a player exemplifies strategic gameplay as they cautiously peek around corners, closely monitoring enemy movements. With only 30 seconds left on the clock, they spot an opponent, make a precise shot, and secure a headshot. The score is tight at 4-5, intensifying the stakes. \n",
      "\n",
      "As the player maneuvers through the map, they utilize cover effectively and prepare for a possible ambush. In a climactic moment, they execute a decisive play right before time runs out, clinching a clutch moment that showcases their skill and composure under pressure.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import base64\n",
    "import openai\n",
    "import time\n",
    "from pathlib import Path\n",
    "import whisper\n",
    "from moviepy.editor import *\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"\")\n",
    "\n",
    "def extract_frames(video_path, output_folder, frame_interval=60):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return []\n",
    "\n",
    "    frame_count = 0\n",
    "    extracted_frame_paths = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_count % frame_interval == 0:\n",
    "            frame_filename = f\"{output_folder}/frame_{frame_count}.jpg\"\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "            extracted_frame_paths.append(frame_filename)\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Frame extraction complete. {len(extracted_frame_paths)} frames extracted.\")\n",
    "\n",
    "    return extracted_frame_paths\n",
    "\n",
    "def get_video_duration(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return 0\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)  # Get frame rate\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # Get total frame count\n",
    "\n",
    "    duration = round(frame_count / fps) if fps > 0 else 0  # Round the duration\n",
    "\n",
    "    cap.release()\n",
    "    return duration\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "def get_frame_descriptions(frame_paths, final_prompt, max_retries=3, retry_delay=2):\n",
    "    base64_frames = [image_to_base64(path) for path in frame_paths]\n",
    "\n",
    "    prompt_messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                *map(lambda x: {\"image\": x, \"resize\": 768}, base64_frames),\n",
    "                final_prompt\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    params = {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": prompt_messages,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = client.chat.completions.create(**params)\n",
    "            return result.choices[0].message.content\n",
    "        except openai.InternalServerError as e:\n",
    "            print(f\"Server error on attempt {attempt + 1}: {e}. Retrying after {retry_delay} seconds...\")\n",
    "            time.sleep(retry_delay)\n",
    "\n",
    "    print(\"Failed to obtain descriptions after several retries.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# this is only in version2\n",
    "# Function to extract audio from video\n",
    "def extract_audio(video_path, audio_output_path):\n",
    "  video_clip = VideoFileClip(video_path)\n",
    "  video_clip.audio.write_audiofile(audio_output_path)\n",
    "\n",
    "\n",
    "# this is only in version2 \n",
    "# Function to transcribe audio using Whisper(this is an API from OpenAI)\n",
    "def transcribe_audio_with_whisper(audio_file):\n",
    "  model = whisper.load_model(\"base\")\n",
    "  result = model.transcribe(audio_file)\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "# Main execution\n",
    "# video_path = \"video.mp4\"\n",
    "video_path = \"v2.mp4\"\n",
    "output_folder = \"extracted_frames\"\n",
    "\n",
    "# Calculate video duration and adjust prompt\n",
    "video_duration = get_video_duration(video_path)\n",
    "print(f\"Video duration: {video_duration} seconds.\")\n",
    "\n",
    "word_count = video_duration * 2.2 # make it 2, 2.2 and 2.5 \n",
    "print(f\"Word Count: {word_count}\")\n",
    "\n",
    "prompt = f\"(This video is ONLY {video_duration} seconds long, so make sure the voiceover MUST be less than {word_count} words)\"\n",
    "\n",
    "# final_prompt = \"ACT as an commentator. In a conversational style, explain step-by-step what is happening in match the frames suitable for a voiceover.\" + prompt\n",
    "# promt can be act as an engaging commentator\n",
    "final_prompt = \"ACT as an commentator. In a conversational style, explain step-by-step what is happening in valorant game the frames suitable for a voiceover.\" + prompt\n",
    "\n",
    "# Extract frames \n",
    "extracted_frames = extract_frames(video_path, output_folder)\n",
    "print(\"extracted_frames\",extracted_frames)\n",
    "\n",
    "# Extract audio from the video\n",
    "audio_output_path = 'extracted_audio.mp3'\n",
    "extract_audio(video_path, audio_output_path)\n",
    "\n",
    "\n",
    "# Transcribe audio\n",
    "audio_transcription = transcribe_audio_with_whisper(audio_output_path)\n",
    "\n",
    "\n",
    "# get dget_frame_descriptions\n",
    "descriptions = get_frame_descriptions(extracted_frames, final_prompt)\n",
    "\n",
    "print(descriptions)\n",
    "\n",
    "\n",
    "\n",
    "# Combine audio transcription with visual descriptions\n",
    "combined_text = f\"Video Description:\\n{descriptions}\\n\\nAudio Transcription:\\n{audio_transcription}\"\n",
    "\n",
    "# Function to get rewritten description from Chat Completions API\n",
    "def get_rewritten_description(text):\n",
    "  response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": \"Use the Video Description to describe what happening in the video and also use the Audio Transcription to complement the analysis into a spoken report\"+ prompt},\n",
    "      {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "  )\n",
    "\n",
    "  # Corrected method to extract the content from the response\n",
    "  revised_text = response.choices[0].message.content\n",
    "  return revised_text\n",
    "\n",
    "# Get revised description\n",
    "revised_description = get_rewritten_description(combined_text)\n",
    "print(\"Revised Description from Chat Completions API:\")\n",
    "print(revised_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f67c479e-1e81-4724-b2c1-ffe3f4f651c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_voiceover(text, output_audio_path, model=\"tts-1\", voice=\"echo\"):\n",
    "    response = client.audio.speech.create(\n",
    "        model=model,\n",
    "        voice=voice,\n",
    "        input=text\n",
    "    )\n",
    "    response.stream_to_file(Path(output_audio_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64ddc818-8f16-40c7-b654-d8b1e7a932cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip, AudioFileClip\n",
    "\n",
    "# Function to merge audio with video\n",
    "def merge_audio_video(video_path, audio_path, output_video_path):\n",
    "    video_clip = VideoFileClip(video_path)\n",
    "    audio_clip = AudioFileClip(audio_path)\n",
    "\n",
    "    final_clip = video_clip.set_audio(audio_clip)\n",
    "    final_clip.write_videofile(output_video_path, codec=\"libx264\", audio_codec=\"aac\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fa536f5-0988-48ca-938a-b675d44be594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video openai2.mp4.\n",
      "MoviePy - Writing audio in openai2TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video openai2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready openai2.mp4\n"
     ]
    }
   ],
   "source": [
    "if extracted_frames:\n",
    "    # Create and save voiceover\n",
    "    output_audio_path = 'voiceover.mp3'\n",
    "    create_voiceover(descriptions, output_audio_path)\n",
    "\n",
    "    # Merge audio with video and save as new file\n",
    "    output_video_path = 'openai2.mp4'\n",
    "    merge_audio_video(video_path, output_audio_path, output_video_path)\n",
    "else:\n",
    "    print(\"No frames were extracted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5f28d1-902a-4207-a18e-c54a19a63340",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
